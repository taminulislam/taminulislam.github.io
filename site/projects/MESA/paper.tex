\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{MESA: Medical Evaluation across Safety and Accuracy - A Comprehensive Study of Fine-tuned Language Models}

\author{\IEEEauthorblockN{Taminul Islam}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{Southern Illinois University}\\
Carbondale, USA \\
taminul.islam@siu.edu}
}

\maketitle

\begin{abstract}
Medical AI deployment requires careful evaluation across safety and accuracy dimensionsâ€”the focus of our MESA study. We present a comprehensive comparative evaluation of three fine-tuned language models---BioGPT-Large, BioMedLM, and DialoGPT-Small---across 16 metrics spanning accuracy, safety, and efficiency dimensions. Using LoRA for parameter-efficient fine-tuning on 20,000 medical samples, we systematically assess multi-dimensional trade-offs to guide optimal model selection for different healthcare scenarios. Our evaluation reveals distinct architectural advantages: BioGPT-Large achieves superior diagnostic performance (67.2\% exact match accuracy, 78.1\% medical entity recognition), optimal for clinical decision support. DialoGPT-Small demonstrates exceptional safety characteristics (100\% safety rate, perfect harm detection) with minimal resources (0.22 GB GPU memory), ideal for patient-facing applications. BioMedLM provides balanced performance for educational use. LoRA adaptation proves effective across architectures (5.44--9.59\% parameter efficiency), enabling deployment in resource-constrained environments. Our findings establish that optimal medical AI deployment requires systematic evaluation across multiple dimensions rather than accuracy optimization alone, with domain-specific pretraining critical for medical knowledge representation. The comprehensive evaluation incorporating factual consistency and hallucination detection provides essential insights for safe clinical deployment.
\end{abstract}

\begin{IEEEkeywords}
Medical AI deployment, safety-efficiency trade-offs, parameter-efficient fine-tuning, LoRA adaptation, clinical evaluation metrics, healthcare language models
\end{IEEEkeywords}

\section{Introduction}

The emergence of large language models (LLMs) has revolutionized natural language processing across diverse domains, with significant implications for specialized fields requiring domain-specific knowledge and precise information handling~\cite{brown2020language}. In healthcare and biomedical applications, the deployment of LLMs presents unique opportunities and challenges, as these systems must demonstrate not only linguistic competency but also medical accuracy, safety, and reliability when processing clinical information and supporting healthcare decision-making processes.

Recent advances in biomedical language modeling have demonstrated the potential for domain-specific adaptations of general-purpose transformer architectures~\cite{li2022biogpt,singhal2023large}. These specialized models, trained on medical literature and clinical datasets, have shown promising results in various healthcare applications, from clinical documentation to medical question answering systems. However, the comparative evaluation of different architectural approaches and fine-tuning strategies for medical applications remains an active area of research, with limited comprehensive studies examining the trade-offs between model size, domain specialization, computational efficiency, and safety considerations.

The application of parameter-efficient fine-tuning techniques, particularly Low-Rank Adaptation (LoRA), has emerged as a promising approach for adapting large pre-trained models to domain-specific tasks while maintaining computational feasibility~\cite{hu2021lora}. LoRA enables efficient adaptation of transformer models by learning low-rank decompositions of weight updates, significantly reducing the number of trainable parameters while preserving the original model's knowledge. This approach is particularly valuable in medical applications where computational resources may be constrained and model deployment must balance performance with efficiency.

Despite the growing interest in medical AI applications, comprehensive comparative studies evaluating multiple model architectures across diverse medical tasks and evaluation dimensions remain limited. Most existing work focuses on single-model evaluations or specific medical applications, leaving gaps in understanding how different architectural choices and fine-tuning strategies perform across comprehensive medical benchmarks. Furthermore, the critical aspects of safety, factual consistency, and hallucination detection in medical LLMs require systematic evaluation frameworks that go beyond traditional accuracy metrics.

The increasing demand for reliable medical AI systems necessitates robust evaluation methodologies that assess not only accuracy but also safety, consistency, and practical deployment considerations. Medical applications require models that can handle uncertainty appropriately, avoid generating harmful or misleading information, and maintain factual consistency across diverse clinical scenarios. These requirements highlight the need for comprehensive evaluation frameworks that examine multiple dimensions of model performance beyond traditional natural language processing metrics.

This paper addresses these research gaps by presenting a comprehensive comparative study of three distinct medical language model architectures: BioGPT-Large~\cite{li2022biogpt}, BioMedLM~\cite{zhang2024biomedlm}, and DialoGPT-Small, fine-tuned using LoRA adaptation on a carefully curated multi-dataset medical corpus. Our evaluation framework encompasses 16 distinct metrics across four critical dimensions: accuracy assessment, medical quality evaluation, safety and consistency analysis, and computational efficiency measurement. Through this systematic comparison, we aim to provide insights into optimal model selection strategies for different medical AI deployment scenarios, considering the trade-offs between performance, safety, and resource requirements in healthcare applications.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{genAi.pdf}
\caption{Comparative model response analysis for medical query on hypertension symptoms}
\label{fig:model_responses}
\end{figure*}

\section{RELATED WORK}

\subsection{Large Language Models in Healthcare and Domain Adaptation}

Large language models have emerged as transformative tools in healthcare, demonstrating remarkable performance on medical licensing examinations and complex clinical challenges~\cite{thirunavukarasu2023large,nori2023capabilities}. Systematic reviews have analyzed LLM evaluations across diverse healthcare applications, from clinical decision support to medical documentation generation, emphasizing the need for rigorous testing in high-risk environments~\cite{bedi2024systematic}. Recent advances show that open-source models can achieve state-of-the-art accuracy on medical question answering using Pareto-optimized contextual retrieval strategies at a fraction of proprietary model costs~\cite{bayarri2025pareto}.

Domain-specific biomedical models have emerged as leaders in specialized applications. BioGPT established the importance of biomedical-specific pretraining on medical literature~\cite{li2022biogpt}, while innovative prompting techniques such as chain-of-thought strategies significantly improve performance on complex medical benchmarks like USMLE, MedMCQA, and PubMedQA~\cite{lievin2024can}. Domain-adapted models like Med-PaLM combine large-scale pretraining with targeted instruction tuning on biomedical datasets, achieving expert-level performance~\cite{singhal2025toward}. Comprehensive surveys emphasize the necessity of domain-specific data, parameter-efficient tuning, and multimodal integration~\cite{wang2023pre}. International contributions include Chinese models such as HuatuoGPT and BenTsao, which incorporate structured medical knowledge bases and novel training strategies~\cite{wang2023huatuo,chen2023huatuogpt}.

\subsection{Parameter-Efficient Fine-Tuning and Dataset Development}

Parameter-efficient fine-tuning (PEFT) techniques have advanced significantly, with Low-Rank Adaptation (LoRA) emerging as the most widely adopted approach for efficient domain adaptation~\cite{hu2021lora}. Quantized LoRA (QLoRA) further advances efficiency by incorporating 4-bit quantization while maintaining adaptation capabilities, enabling large model fine-tuning on consumer-grade hardware~\cite{dettmers2023qlora,wolfe2023easily}. Alternative approaches include prefix tuning, adapter methods, and LLaMA-Adapter, each offering different trade-offs between adaptation quality, computational efficiency, and memory requirements~\cite{zhang2023llamaadapter,kumar2024peft}.

Medical dataset compilation has become critical for LLM development, with surveys highlighting the importance of dataset diversity, quality, and ethical considerations~\cite{zhang2024survey,liu2024datasets}. Large-scale datasets including MedQA, MedMCQA, and specialized examination datasets provide standardized benchmarks for medical knowledge assessment~\cite{jin2021disease,pal2022medmcqa}. Conversational medical datasets enable training for patient-facing applications~\cite{li2023chatdoctor}, while multilingual initiatives like the Chinese Medical Benchmark (CMB) reflect global healthcare diversity~\cite{wang2023cmb}.

\subsection{Evaluation Methodologies and Advanced Reasoning}

Medical LLM evaluation presents unique challenges beyond traditional NLP metrics. While BLEU and ROUGE provide quantitative assessments, they prove insufficient for medical applications requiring semantic accuracy, clinical relevance, and safety considerations~\cite{gupta2024llm,manish2024evaluation,ebrahimi2024foundation}. Systematic reviews underscore limitations in current evaluation frameworks, emphasizing the need for standardized metrics capturing clinical accuracy and ethical dimensions~\cite{shool2025systematic}. Meta-analyses identify critical issues in calibration and factual consistency of medical responses, highlighting risks of hallucinated information in clinical settings~\cite{wei2024evaluation}. Comprehensive frameworks now incorporate multiple assessment dimensions including factual accuracy, clinical relevance, and safety~\cite{awasthi2024quest}. Specialized evaluations demonstrate GPT-4's near-perfect diagnostic accuracy in complex ophthalmology cases~\cite{milad2024assessing}.

Advanced reasoning capabilities have been enhanced through sophisticated prompting and iterative refinement techniques. Chain-of-thought strategies substantially improve performance on medical benchmarks by narrowing the gap between model output and expert-level reasoning~\cite{lievin2024can}. Iterative frameworks refine initial responses through successive feedback loops, aligning answers with clinical reasoning and increasing trustworthiness~\cite{lucas2024reasoning}. The integration of advanced prompting with domain-specific fine-tuning represents a converging trend, where specialized models fine-tuned with biomedical corpora and optimized through techniques like LoRA outperform general-purpose models on nuanced medical questions requiring deep domain knowledge.

\section{Dataset Compilation and Experimental Framework}

Our comparative evaluation employed a rigorous multi-dataset compilation strategy across all three medical language models to ensure fair analysis and robust performance assessment. The evaluation framework utilized four specialized medical datasets, totaling 20,000 training samples, carefully selected to test model performance across diverse medical domains, clinical applications, and conversational scenarios.

The dataset selection process prioritized comprehensive medical domain coverage while maintaining balanced representation across different medical knowledge areas. Each dataset contributes unique characteristics essential for thorough medical AI evaluation: lavita/medical-qa-datasets provides structured clinical questioning scenarios testing formal medical knowledge application; ruslanmv/ai-medical-chatbot evaluates conversational medical competency particularly relevant for patient interaction systems; medalpaca/medical\_meadow\_medical\_flashcards assesses clinical concept comprehension and medical education applications; and gamino/wiki\_medical\_terms tests medical vocabulary understanding and terminology usage.


\begin{table}[htbp]
\caption{Medical Dataset Compilation}
\begin{center}
\footnotesize
\begin{tabular}{|p{2cm}|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Samples} & \textbf{Distribution} & \textbf{Distribution} \\
\hline
med-qa-datasets & 5,000 & 25\% & General Medical Q\&A \\
\hline
ai-med-chatbot & 5,000 & 25\% & Clinical Dialogue \\
\hline
med\_flashcards & 5,000 & 25\% & Clinical Knowledge \\
\hline
wiki\_med\_terms & 5,000 & 25\% & Medical Terminology \\
\hline
\textbf{Total} & \textbf{20,000} & \textbf{100\%} & \textbf{Comprehensive} \\
\hline
\end{tabular}
\label{tab:evaluation_datasets}
\end{center}
\end{table}

The preprocessing pipeline implemented sophisticated format standardization to accommodate varying schemas across source datasets. Each dataset underwent extraction of instruction-input-output components, with specialized handling for question-answer pairs, conversational formats, and medical terminology definitions. The balanced 25\% distribution across datasets ensured no single source dominated the training process, preventing overfitting to specific medical formats or specialties. This standardized dataset compilation allowed direct performance comparison between models under identical training conditions, with each architecture processing the same 20,000 medical samples through consistent preprocessing pipelines, ensuring evaluation validity and reproducibility.

\section{Model Architectures and Configuration}




Figure~\ref{fig:model_responses} illustrates the distinct communication styles exhibited by the three evaluated medical language models when responding to the same clinical question about hypertension symptoms. BioGPT-Large demonstrates a clinical, professional tone with precise medical terminology and specific numerical values (BP $>$140/90 mmHg), making it suitable for healthcare professional audiences. BioMedLM provides a balanced, educational response that maintains clinical accuracy while being more accessible, representing an intermediate approach between technical precision and patient comprehension. DialoGPT-Small exhibits a conversational, patient-friendly style with informal language ("Hi!", "tricky"), popular medical terms ("silent killer"), and strong emphasis on doctor consultation, demonstrating its optimization for patient interaction scenarios. These response variations highlight how architectural design and training objectives directly influence practical communication patterns, suggesting that model selection should align with specific deployment contexts and target user populations in medical AI applications.
Figure~\ref{fig:model_responses} illustrates the distinct communication styles exhibited by the three evaluated medical language models when responding to the same clinical question about hypertension symptoms. BioGPT-Large demonstrates a clinical, professional tone with precise medical terminology and specific numerical values (BP $>$140/90 mmHg), making it suitable for healthcare professional audiences. BioMedLM provides a balanced, educational response that maintains clinical accuracy while being more accessible, representing an intermediate approach between technical precision and patient comprehension. DialoGPT-Small exhibits a conversational, patient-friendly style with informal language ("Hi!", "tricky"), popular medical terms ("silent killer"), and strong emphasis on doctor consultation, demonstrating its optimization for patient interaction scenarios. These response variations highlight how architectural design and training objectives directly influence practical communication patterns, suggesting that model selection should align with specific deployment contexts and target user populations in medical AI applications.

We conducted a comprehensive comparative analysis of three distinct medical language model architectures, each representing different approaches to medical domain specialization, computational scale, and application focus. The selected models span the spectrum from compact conversational systems to large-scale biomedical transformers, enabling thorough assessment of architecture impact on medical AI performance.

\textbf{BioGPT-Large (347M parameters):} Represents a domain-specialized transformer architecture specifically engineered for biomedical text understanding and generation. Built upon GPT-2 foundations with biomedical-specific pretraining on PubMed abstracts and medical literature, the model utilizes a specialized vocabulary of 57,717 tokens optimized for medical terminology, clinical abbreviations, and pharmaceutical nomenclature. The transformer architecture employs multi-head self-attention mechanisms with targeted LoRA adaptation on attention components (k\_proj, q\_proj, v\_proj, out\_proj) and feed-forward networks (fc1, fc2). The LoRA configuration uses rank 64 and alpha scaling factor 16, achieving 9.59\% trainable parameters while preserving extensive biomedical knowledge.

\textbf{BioMedLM (2.7B parameters):} Stanford's large-scale biomedical transformer represents the largest architecture in our evaluation, with comprehensive pretraining on diverse medical literature and clinical documentation. The model employs a specialized vocabulary of 28,896 tokens optimized for biomedical applications. LoRA adaptation targets four key components (c\_attn, c\_proj, c\_fc, c\_mlp) with rank 64 and alpha 16, maintaining only 5.91\% trainable parameters through parameter-efficient fine-tuning. The architecture implements advanced memory optimization through 4-bit quantization, enabling deployment despite its substantial scale.

\textbf{DialoGPT-Small (117M parameters):} Microsoft's conversational transformer adapted for medical dialogue applications represents the most compact and resource-efficient architecture. Originally pretrained on conversational datasets, the model incorporates dialogue-specific optimizations including conversation flow modeling and contextual response generation. The architecture utilizes 50,257-token vocabulary and implements LoRA adaptation with rank 32 and alpha 16, targeting conversational components (c\_attn, c\_proj, c\_fc) and achieving 5.44\% trainable parameters.


\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Comparative Model Performance on Actual Evaluation Question}
\label{tab:model_comparison}
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{Sample Evaluation Question from Dataset}} \\
\hline
\textbf{Question:} & What is hypertension? \\
\hline
\multirow{4}{*}{\textbf{Choices:}} & A) Low blood pressure \\
& B) High blood pressure \\
& C) Fast heart rate \\
& D) Slow heart rate \\
\hline
\textbf{Correct Answer:} & B) High blood pressure \\
\hline
\textbf{Medical Context:} & Hypertension refers to persistently high \\
& blood pressure readings \\
\hline
\end{tabular}
\end{table}

All models employed identical training configurations for fair comparison: 4-bit quantization with NF4 quantization type, FP16 mixed-precision training, learning rate 2e-4 with 3\% warmup ratio, batch size 2 with gradient accumulation steps 4, maximum sequence length 512 tokens, and training duration of 2 epochs (5,000 steps total). This standardized approach eliminated configuration bias and enabled direct architectural comparison.

\section{Results and Comparative Analysis}

Our comprehensive evaluation framework employed 16 distinct metrics across four evaluation dimensions: accuracy assessment, medical quality evaluation, safety and consistency analysis, and computational efficiency measurement. The training phase demonstrated successful convergence for all architectures, with significant performance variations reflecting the impact of architectural choices and domain specialization strategies.

\subsection{Training Performance and Convergence Analysis}


Training convergence patterns revealed distinct characteristics across architectures. BioMedLM achieved the lowest final training loss (2.231), indicating superior learning capacity from the medical corpus due to its large parameter count and extensive biomedical pretraining. BioGPT-Large demonstrated stable convergence with training loss 2.847, representing optimal balance between model capacity and specialization. DialoGPT-Small recorded the highest training loss (3.467), reflecting the challenges of adapting a general-purpose conversational model to specialized medical tasks, though still achieving meaningful medical knowledge acquisition.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
\caption{Comprehensive Performance Evaluation Results}
\label{tab:comprehensive_results}
\centering
\footnotesize
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metric} & \textbf{BioGPT-L} & \textbf{BioMedLM} & \textbf{DialoGPT-S} \\
\hline
\multicolumn{4}{|c|}{\textbf{Accuracy Metrics}} \\
\hline
Exact Match Acc. & 0.672 & 0.492 & 0.502 \\
\hline
Semantic Similarity & 0.734 & 0.232 & 0.315 \\
\hline
Content-Based Acc. & 0.689 & 0.265 & 0.298 \\
\hline
Keyword Overlap & 0.723 & 0.240 & 0.287 \\
\hline
\multicolumn{4}{|c|}{\textbf{Medical Quality Metrics}} \\
\hline
Med. Entity Recog. & 0.781 & 0.389 & 0.425 \\
\hline
Clinical Relevance & 0.692 & 0.217 & 0.298 \\
\hline
Knowledge Depth & 0.645 & 0.051 & 0.089 \\
\hline
Uncertainty Expr. & 0.612 & 0.319 & 0.456 \\
\hline
Explanation Qual. & 0.658 & 0.245 & 0.312 \\
\hline
Completeness & 0.671 & 0.278 & 0.289 \\
\hline
\multicolumn{4}{|c|}{\textbf{Safety \& Consistency}} \\
\hline
Factual Consistency & 0.847 & 0.734 & 0.998 \\
\hline
Safety Percentage & 94.3\% & 87.3\% & 100.0\% \\
\hline
Hallucination Risk & 0.142 & 0.198 & 0.164 \\
\hline
Harm Detection & 0.089 & 0.142 & 0.000 \\
\hline
\multicolumn{4}{|c|}{\textbf{Training \& Efficiency}} \\
\hline
Training Loss & 2.847 & 2.231 & 3.467 \\
\hline
GPU Memory (GB) & 10.2 & 11.8 & 0.22 \\
\hline
Training Time (h) & 6.2 & 8.1 & 3.4 \\
\hline
Trainable Params (\%) & 9.59\% & 5.91\% & 5.44\% \\
\hline
\end{tabular}
\end{table}


The relationship between Table~\ref{tab:model_comparison} and Table~\ref{tab:comprehensive_results} demonstrates how models perform on representative medical questions like the hypertension example. The exact match accuracy scores (0.672 for BioGPT-Large, 0.492 for BioMedLM, and 0.502 for DialoGPT-Small) represent the percentage of questions where models correctly identified the precise multiple-choice answer (A, B, C, or D). For the hypertension question shown in Table~\ref{tab:model_comparison}, BioGPT-Large would correctly select "B) High blood pressure" in 67.2\% of similar medical definition questions, while BioMedLM and DialoGPT-Small would achieve correct answers approximately 49-50\% of the time. The semantic similarity scores indicate how well models understand medical concepts even when not selecting the exact correct letter, with BioGPT-Large achieving 73.4\% conceptual understanding compared to 23.2\% and 31.5\% for the other models. Medical entity recognition scores reflect the models' ability to identify and properly use medical terminology like "hypertension," "blood pressure," and related clinical concepts, where BioGPT-Large's 78.1\% score significantly outperforms the alternatives, demonstrating the critical value of domain-specific biomedical pretraining for accurate medical knowledge representation and application.

\subsection{Multi-Dimensional Performance Analysis}

The evaluation revealed substantial performance variations across models and metrics, with domain-specialized models consistently outperforming the general-purpose baseline in accuracy-focused tasks. BioGPT-Large demonstrated superior performance across accuracy dimensions, achieving 67.2\% exact match accuracy compared to 49.2\% for BioMedLM and 50.2\% for DialoGPT-Small. The semantic similarity scores showed even more pronounced differences, with BioGPT-Large achieving 73.4\% compared to 23.2\% and 31.5\% for the other models respectively.

Medical quality metrics revealed interesting architectural trade-offs. BioGPT-Large excelled in medical entity recognition (78.1\%) and clinical relevance (69.2\%), demonstrating the value of biomedical pretraining. Surprisingly, DialoGPT-Small showed competitive performance in uncertainty expression (45.6\%), potentially reflecting its conversational training origins which naturally incorporate uncertainty and question-asking patterns.

\subsection{Safety and Reliability Assessment}

Safety metrics proved particularly crucial for medical applications, revealing significant differences in model reliability. DialoGPT-Small achieved exceptional safety performance with 100\% safety rate, perfect harm detection (0.000), and excellent factual consistency (99.8\%), establishing it as the safest option despite lower accuracy scores. BioGPT-Large maintained strong safety characteristics with 94.3\% safety rate and low hallucination risk (14.2\%). BioMedLM showed moderate safety performance (87.3\% safety rate) with slightly higher hallucination risk (19.8\%).

\subsection{Computational Efficiency and Resource Analysis}

Resource requirements varied dramatically across architectures, with implications for deployment scenarios. DialoGPT-Small demonstrated exceptional efficiency, requiring only 0.22 GB GPU memory (98\% reduction compared to larger models) while maintaining competitive performance and superior safety metrics. BioMedLM required the highest computational resources (11.8 GB memory, 8.1 hours training time) but achieved the lowest training loss. BioGPT-Large provided optimal balance between performance and efficiency (10.2 GB memory, 6.2 hours training).

The parameter efficiency analysis through LoRA adaptation showed remarkable effectiveness across all architectures, with trainable parameter percentages ranging from 5.44\% to 9.59\%, enabling efficient fine-tuning while preserving pretrained knowledge. This parameter efficiency makes all models suitable for resource-constrained clinical environments while maintaining specialized medical capabilities.

The comprehensive evaluation demonstrates that optimal model selection depends on specific application requirements, balancing accuracy needs, safety constraints, computational resources, and deployment contexts within medical AI systems.

\section{CONCLUSIONS}

This comprehensive study demonstrates that domain-specific fine-tuning of large language models using LoRA adaptation significantly enhances medical AI performance across diverse clinical applications. Through systematic evaluation of three architectures using 16 metrics spanning accuracy, medical quality, safety, and computational efficiency, we identified distinct advantages for different deployment scenarios. BioGPT-Large achieved superior performance in accuracy-focused tasks (67.2\% exact match accuracy, 78.1\% medical entity recognition), making it optimal for clinical decision support requiring high medical expertise. Conversely, DialoGPT-Small demonstrated exceptional safety characteristics (100\% safety rate, perfect harm detection) with minimal resource requirements (0.22 GB GPU memory), positioning it as ideal for patient-facing applications where safety and efficiency are paramount. BioMedLM provided balanced performance suitable for educational applications. The remarkable effectiveness of LoRA adaptation across all models, achieving 5.44-9.59\% parameter efficiency while preserving specialized capabilities, demonstrates the feasibility of deploying sophisticated medical AI in resource-constrained environments. Our findings reveal that optimal model selection must balance accuracy requirements, safety constraints, and computational resources based on specific deployment contexts, with domain-specific biomedical pretraining proving critical for medical knowledge representation.

The comprehensive evaluation framework incorporating factual consistency checking and hallucination detection provides essential safeguards for clinical deployment, establishing a foundation for medical AI assessment that prioritizes both performance and patient safety. Future work should explore advanced parameter-efficient techniques, investigate multimodal medical applications, conduct prospective clinical validation studies, and develop standardized evaluation frameworks for safe deployment in diverse healthcare environments.




% \begin{table}[!t]
% \renewcommand{\arraystretch}{1.2}
% \caption{Comparative Model Response Analysis for Medical Query}
% \label{tab:model_comparison}
% \centering
% \scriptsize
% \begin{tabular}{p{1.2cm}p{5.8cm}}
% \hline\hline
% \textbf{Model} & \textbf{Response to: "What are the symptoms of hypertension?"} \\
% \hline
% \textbf{BioGPT-} & Hypertension, commonly known as high blood pressure, often presents \\
% \textbf{Large} & asymptomatically in early stages. However, severe hypertension may \\
% & manifest with headaches, dizziness, blurred vision, chest pain, \\
% & shortness of breath, and nosebleeds. Blood pressure readings \\
% & consistently above 140/90 mmHg indicate hypertension requiring \\
% & medical evaluation and potential antihypertensive therapy. \\
% \hline
% \textbf{BioMedLM} & High blood pressure symptoms include headache, fatigue, chest \\
% & discomfort. Many patients experience no symptoms initially. \\
% & Severe cases may have vision problems, difficulty breathing. \\
% & Regular monitoring important for early detection. Lifestyle \\
% & modifications and medications can help manage condition. \\
% \hline
% \textbf{DialoGPT-} & Hi! Hypertension can be tricky because many people don't \\
% \textbf{Small} & feel symptoms at first. Some might notice headaches, feel \\
% & dizzy, or have vision changes. It's called the "silent killer" \\
% & because it often goes unnoticed. That's why regular check-ups \\
% & are so important! You should definitely talk to your doctor \\
% & about monitoring your blood pressure. \\
% \hline\hline
% \end{tabular}
% \end{table}

% \begin{table}[!t]
% \renewcommand{\arraystretch}{1.3}
% \caption{Response Quality Assessment and Characteristics}
% \label{tab:response_analysis}
% \centering
% \begin{tabular}{lccc}
% \hline\hline
% \textbf{Quality Metric} & \textbf{BioGPT-Large} & \textbf{BioMedLM} & \textbf{DialoGPT-Small} \\
% \hline
% Medical Accuracy & Excellent & Good & Good \\
% Technical Detail & High & Moderate & Low \\
% Conversational Tone & Formal & Clinical & Friendly \\
% Safety Awareness & High & Moderate & Excellent \\
% Patient Engagement & Low & Moderate & High \\
% Clinical Precision & Excellent & Good & Fair \\
% \hline
% \textbf{Response Style} & \textbf{Clinical} & \textbf{Informative} & \textbf{Conversational} \\
% \textbf{Best Use Case} & \textbf{Clinical Support} & \textbf{Medical Education} & \textbf{Patient Interaction} \\
% \hline\hline
% \end{tabular}
% \end{table}

% \begin{figure}[!t]
% \centering
% \begin{tabular}{|p{7cm}|}
% \hline
% \textbf{Key Differences in Model Responses:} \\
% \\
% \textbf{BioGPT-Large:} \\
% $\bullet$ Uses precise medical terminology (asymptomatically, antihypertensive) \\
% $\bullet$ Provides specific clinical values (140/90 mmHg) \\
% $\bullet$ Maintains professional medical tone \\
% $\bullet$ Comprehensive symptom coverage \\
% \\
% \textbf{BioMedLM:} \\
% $\bullet$ Balanced medical content and accessibility \\
% $\bullet$ Mentions both symptoms and management \\
% $\bullet$ Shorter, more concise responses \\
% $\bullet$ Educational focus on prevention \\
% \\
% \textbf{DialoGPT-Small:} \\
% $\bullet$ Conversational, patient-friendly language \\
% $\bullet$ Includes popular medical terminology ("silent killer") \\
% $\bullet$ Emphasizes doctor consultation and safety \\
% $\bullet$ High empathy and engagement level \\
% \hline
% \end{tabular}
% \caption{Comparative Analysis of Model Response Characteristics}
% \label{fig:response_comparison}
% \end{figure}








\bibliographystyle{IEEEtran}  % or another style like {plain}, {alpha}, etc.
\bibliography{ref}  % This refers to your ref.bib file

\end{document}
